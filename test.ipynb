{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#####################\n",
    "# YOU MUST WRITE YOUR STUDENT ID IN THE VARIABLE STUDENT_ID\n",
    "# EXAMPLE: STUDENT_ID = \"12345678\"\n",
    "#####################\n",
    "STUDENT_ID = \"20251189\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df1, df2):\n",
    "    # BUT you should keep the file name as \"{STUDENT_ID}_simple_seq.p#.answer.csv\"\n",
    "    df1.to_csv(f'{STUDENT_ID}_simple_seq.p1.answer.csv')\n",
    "    df2.to_csv(f'{STUDENT_ID}_simple_seq.p2.answer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.W = torch.randn(in_features, out_features, dtype=torch.float32, requires_grad=True)\n",
    "        self.b = torch.randn(out_features, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.mm(self.W) + self.b\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.layer1 = CustomLinear(input_dim, 1000)\n",
    "        self.layer2 = CustomLinear(1000, 100)\n",
    "        self.layer3 = CustomLinear(100, 19)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1.forward(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2.forward(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer3.forward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2     3     4    5    6     7     8     9    10    11    12  \\\n",
      "0   W25   W26   W27   W19   W28  W29  W30   W31   W32   W33  W34   W35   W36   \n",
      "1   W41    W4   W42   W43   W44  W45  W46   W47   W48   W49  W50   W51   W52   \n",
      "2   W55   W19   W46   W32   W32  W56  W57   W58   W59   W19  W13   W60   W19   \n",
      "3   W13   W83   W32   W32   W56  W57  W13   W84   W19   W28  W85   W86   W24   \n",
      "4   W87   W88   W89   W90   W32  W91  W13   W92   W93   W90  W94   W95   W24   \n",
      "5   W13   W52   W32   W53   W17  W13  W96   W97   W10    W2  W98   W99   W19   \n",
      "6  W122  W123  W110  W124  W125  W19  W13  W126  W127  W128  W32  W129  W130   \n",
      "\n",
      "    13    14    15    16   17    18   19  \n",
      "0  W37   W38   W39   W24  W40   PAD  PAD  \n",
      "1  W53   W17   W54   W24  PAD   PAD  PAD  \n",
      "2  W13   W61   W62   PAD  PAD   PAD  PAD  \n",
      "3  PAD   PAD   PAD   PAD  PAD   PAD  PAD  \n",
      "4  PAD   PAD   PAD   PAD  PAD   PAD  PAD  \n",
      "5  W13  W100   W24   PAD  PAD   PAD  PAD  \n",
      "6  W36   W13  W131  W132  W17  W133  W24  \n",
      "  true_label\n",
      "0        D11\n",
      "1         D1\n",
      "2         D3\n",
      "3        D20\n",
      "4        D20\n",
      "900\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "column_names = [f'{i}' for i in range(1, 22)]\n",
    "train = pd.read_csv('./dataset/simple_seq.train.csv', names=column_names)\n",
    "\n",
    "def extract_last_value(row):\n",
    "    non_nan_values = row.dropna().tolist() \n",
    "    return non_nan_values[-1]\n",
    "\n",
    "true_labels = train.apply(extract_last_value, axis=1) \n",
    "\n",
    "def remove_last_value(row):\n",
    "    non_nan_values = row.dropna().tolist()  \n",
    "    non_nan_values.pop()\n",
    "    return pd.Series(non_nan_values)\n",
    "\n",
    "train = train.apply(remove_last_value, axis=1) \n",
    "train.fillna(\"PAD\", inplace=True)\n",
    "\n",
    "true_labels_df = pd.DataFrame(true_labels, columns=[\"true_label\"])\n",
    "\n",
    "print(train.head(7))\n",
    "print(true_labels_df.head())\n",
    "print(len(train))\n",
    "print(len(true_labels_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1     2      3     4      5      6     7      8      9     10     11  \\\n",
      "0    W13   W81    W19  W346   W846  W1582   W70    W28  W5433    W19  W1163   \n",
      "1  W5413  W111  W5414   W32    W68  W5415   W12  W2402    W19  W5438  W5439   \n",
      "2  W5413  W111  W5414   W32    W68  W5415   W12   W417   W346   W336    W17   \n",
      "3  W5413  W111  W5414   W32    W68  W5415   W12   W346    W32  W2833    W93   \n",
      "4  W5413  W111  W5414   W32    W68  W5415   W12   W111   W346    W47   W336   \n",
      "5  W5413  W111  W5414   W32    W68  W5415   W12   W346   W168  W2464  W5448   \n",
      "6    W87   W31    W47   W38  W1196    W97  W627  W5449    PAD    PAD    PAD   \n",
      "\n",
      "      12     13     14     15     16     17    18     19   20  \n",
      "0  W2261    W24    PAD    PAD    PAD    PAD   PAD    PAD  PAD  \n",
      "1  W5440    W12   W346   W240  W5441  W5442   W24    PAD  PAD  \n",
      "2    W28  W5443    W12   W122    W47    W38  W335  W1248  W24  \n",
      "3    W28  W5444  W5445    W17   W346  W5446   W24    PAD  PAD  \n",
      "4   W286  W5415   W552  W5447   W641   W346   W24    PAD  PAD  \n",
      "5    W24    PAD    PAD    PAD    PAD    PAD   PAD    PAD  PAD  \n",
      "6    PAD    PAD    PAD    PAD    PAD    PAD   PAD    PAD  PAD  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20744\\2865642782.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'PAD' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  test.fillna(\"PAD\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "column = [f'{i}' for i in range(1, 22)]\n",
    "test = pd.read_csv('./dataset/simple_seq.test.csv', header=None, names=column)\n",
    "test.fillna(\"PAD\", inplace=True)\n",
    "test.drop(columns=['21'], inplace=True)\n",
    "print(test.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548\n",
      "['PAD', 'UNK', 'W1', 'W10', 'W100', 'W1003', 'W1004', 'W1008', 'W1009', 'W1010']\n",
      "19\n",
      "['D1', 'D11', 'D12', 'D13', 'D15', 'D16', 'D17', 'D18', 'D19', 'D20', 'D21', 'D27', 'D28', 'D3', 'D32', 'D4', 'D5', 'D6', 'D7']\n"
     ]
    }
   ],
   "source": [
    "unique_words= set(train.values.flatten()).union({\"UNK\"})\n",
    "vocab=sorted(list(unique_words))\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "print(len(vocab))\n",
    "print(vocab[:10])\n",
    "\n",
    "unique_labels = set(true_labels_df['true_label'].values)\n",
    "label_vocab = sorted(list(unique_labels))\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "print(len(label_vocab))\n",
    "print(label_vocab[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(row, vocab_size, word_to_index):\n",
    "    one_hot_matrix = np.zeros((len(row), vocab_size))\n",
    "    for i, word in enumerate(row):\n",
    "        word = word if word in word_to_index else \"UNK\"\n",
    "        if word != \"PAD\":\n",
    "            one_hot_matrix[i, word_to_index[word]] = 1\n",
    "    return one_hot_matrix\n",
    "\n",
    "def one_hot_encode_label(label, label_to_index):   \n",
    "    one_hot_matrix = np.zeros((1, len(label_vocab)))\n",
    "    one_hot_matrix[0, label_to_index[label]] = 1\n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\MLDL\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "X_train_encoded = np.array([one_hot_encode(row, len(vocab), word_to_index) for row in train.values])\n",
    "Y_train_encoded = np.array([one_hot_encode_label(label, label_to_index) for label in true_labels_df[\"true_label\"]])\n",
    "\n",
    "X_train = torch.tensor(X_train_encoded.reshape(X_train_encoded.shape[0], -1), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.argmax(Y_train_encoded, axis=1), dtype=torch.float32)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = CustomModel(input_dim)\n",
    "optimizer = torch.optim.SGD([model.layer1.W, model.layer1.b,\n",
    "                                model.layer2.W, model.layer2.b,\n",
    "                                model.layer3.W, model.layer3.b], lr=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 32\n",
    "dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44%\n"
     ]
    }
   ],
   "source": [
    "train_outputs = model(X_train)\n",
    "predicted_classes = torch.argmax(train_outputs, dim=-1)\n",
    "true_classes = torch.argmax(y_train, dim=-1) \n",
    "\n",
    "correct_predictions = (predicted_classes == true_classes).sum().item()\n",
    "total_samples = true_classes.shape[0]\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
